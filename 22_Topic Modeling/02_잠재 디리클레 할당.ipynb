{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)\n",
    "### 각 토픽당 가장 중요한 단어 5개를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 뉴스 데이터에 대한 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총 11,314개의 뉴스 중 첫번째 뉴스 출력\n",
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# 20개의 카테고리 \n",
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 알파벳을 제외한 구두점, 숫자, 특수 문자를 제거\n",
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 데이터에서 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english') # NLTK로부터 불용어를 받아옵니다.\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [well, sure, story, seem, biased, disagree, st...\n",
       "1    [yeah, expect, people, read, actually, accept,...\n",
       "2    [although, realize, principle, strongest, poin...\n",
       "3    [notwithstanding, legitimate, fuss, proposal, ...\n",
       "4    [well, change, scoring, playoff, pool, unfortu...\n",
       "Name: clean_doc, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 정수 인코딩과 단어 집합 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(52, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 2), (67, 1), (68, 1), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 2), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 2), (86, 1), (87, 1), (88, 1), (89, 1)]\n"
     ]
    }
   ],
   "source": [
    "# 각 단어를 (word_id, word_frequency)의 형태로 변형\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(tokenized_doc)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
    "print(corpus[1])    # 수행된 결과에서 두번째 뉴스 출력. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'faith'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64281"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LDA 모델 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 55s\n",
      "(0, '0.041*\"space\" + 0.016*\"nasa\" + 0.009*\"launch\" + 0.008*\"earth\"')\n",
      "(1, '0.022*\"scsi\" + 0.014*\"tobacco\" + 0.014*\"printer\" + 0.012*\"color\"')\n",
      "(2, '0.013*\"armenian\" + 0.011*\"armenians\" + 0.010*\"israel\" + 0.009*\"turkish\"')\n",
      "(3, '0.011*\"people\" + 0.011*\"would\" + 0.007*\"think\" + 0.006*\"many\"')\n",
      "(4, '0.028*\"game\" + 0.025*\"team\" + 0.020*\"year\" + 0.018*\"games\"')\n",
      "(5, '0.014*\"information\" + 0.009*\"mail\" + 0.009*\"available\" + 0.009*\"list\"')\n",
      "(6, '0.013*\"would\" + 0.012*\"like\" + 0.010*\"know\" + 0.009*\"think\"')\n",
      "(7, '0.014*\"jesus\" + 0.009*\"christian\" + 0.008*\"would\" + 0.008*\"bible\"')\n",
      "(8, '0.011*\"chip\" + 0.011*\"encryption\" + 0.010*\"system\" + 0.009*\"keys\"')\n",
      "(9, '0.023*\"wire\" + 0.017*\"ground\" + 0.016*\"neutral\" + 0.014*\"puck\"')\n",
      "(10, '0.008*\"power\" + 0.006*\"used\" + 0.006*\"high\" + 0.006*\"engine\"')\n",
      "(11, '0.016*\"windows\" + 0.015*\"file\" + 0.014*\"window\" + 0.013*\"files\"')\n",
      "(12, '0.017*\"jpeg\" + 0.012*\"uuencode\" + 0.007*\"guidelines\" + 0.006*\"john\"')\n",
      "(13, '0.036*\"file\" + 0.034*\"output\" + 0.032*\"entry\" + 0.019*\"drive\"')\n",
      "(14, '0.012*\"wave\" + 0.011*\"borland\" + 0.008*\"andrew\" + 0.008*\"paradox\"')\n",
      "(15, '0.028*\"period\" + 0.016*\"play\" + 0.016*\"power\" + 0.013*\"scorer\"')\n",
      "(16, '0.011*\"government\" + 0.011*\"president\" + 0.010*\"people\" + 0.006*\"right\"')\n",
      "(17, '0.030*\"hockey\" + 0.028*\"bike\" + 0.009*\"russia\" + 0.009*\"motorcycle\"')\n",
      "(18, '0.018*\"would\" + 0.015*\"know\" + 0.015*\"like\" + 0.014*\"thanks\"')\n",
      "(19, '0.019*\"card\" + 0.013*\"drive\" + 0.011*\"system\" + 0.010*\"video\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 20 #20개의 토픽, k=20\n",
    "%time \\\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS,\\\n",
    "                                           id2word=dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. LDA 시각화 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pyLDAvis.gensim\n",
    "%matplotlib inline\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 문서 별 토픽 분포 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 문서의 topic 비율은 [(2, 0.29295012), (3, 0.19900966), (6, 0.15044968), (7, 0.1357394), (8, 0.053010184), (10, 0.042731177), (14, 0.015468675), (17, 0.029447285), (19, 0.07214891)]\n",
      "1 번째 문서의 topic 비율은 [(3, 0.10047443), (6, 0.19580537), (7, 0.34584194), (10, 0.040432308), (12, 0.054316122), (18, 0.24469061)]\n",
      "2 번째 문서의 topic 비율은 [(2, 0.23781997), (3, 0.13287808), (6, 0.30617908), (7, 0.31019595)]\n",
      "3 번째 문서의 topic 비율은 [(3, 0.061197326), (6, 0.1789664), (8, 0.59483147), (10, 0.08290129), (12, 0.07018236)]\n",
      "4 번째 문서의 topic 비율은 [(4, 0.21355993), (6, 0.3384961), (12, 0.3489174), (13, 0.06938926)]\n"
     ]
    }
   ],
   "source": [
    "# 상위 5개의 문서에 대해서만 토픽 분포를 확인\n",
    "for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "    if i==5:\n",
    "        break\n",
    "    print(i,'번째 문서의 topic 비율은',topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topic_table_per_doc(ldamodel, corpus, texts):\n",
    "    topic_table = pd.DataFrame()\n",
    "\n",
    "    # 몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내온다.\n",
    "    for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n",
    "        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n",
    "        # 각 문서에 대해서 비중이 높은 토픽순으로 토픽을 정렬한다.\n",
    "        # EX) 정렬 전 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (10번 토픽, 5%), (12번 토픽, 21.5%), \n",
    "        # Ex) 정렬 후 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (12번 토픽, 21.5%), (10번 토픽, 5%)\n",
    "        # 48 > 25 > 21 > 5 순으로 정렬이 된 것.\n",
    "\n",
    "        # 모든 문서에 대해서 각각 아래를 수행\n",
    "        for j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.\n",
    "            if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽\n",
    "                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n",
    "                # 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.\n",
    "            else:\n",
    "                break\n",
    "    return(topic_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문서 번호</th>\n",
       "      <th>가장 비중이 높은 토픽</th>\n",
       "      <th>가장 높은 토픽의 비중</th>\n",
       "      <th>각 토픽의 비중</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2929</td>\n",
       "      <td>[(2, 0.2929493), (3, 0.19900009), (6, 0.150440...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.3458</td>\n",
       "      <td>[(3, 0.100675434), (6, 0.1955454), (7, 0.34582...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.3102</td>\n",
       "      <td>[(2, 0.23782076), (3, 0.13286923), (6, 0.30618...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.5949</td>\n",
       "      <td>[(3, 0.061086643), (6, 0.1791152), (8, 0.59487...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.3489</td>\n",
       "      <td>[(4, 0.21355851), (6, 0.33849853), (12, 0.3489...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.7417</td>\n",
       "      <td>[(7, 0.7417126), (12, 0.058767177), (18, 0.160...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3175</td>\n",
       "      <td>[(3, 0.31747434), (6, 0.1430436), (11, 0.08233...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.4179</td>\n",
       "      <td>[(3, 0.18095222), (6, 0.41792268), (7, 0.19717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>[(3, 0.26006368), (6, 0.55646414), (18, 0.1583...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.4230</td>\n",
       "      <td>[(2, 0.036640637), (6, 0.18301511), (8, 0.0590...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   문서 번호  가장 비중이 높은 토픽  가장 높은 토픽의 비중  \\\n",
       "0      0           2.0        0.2929   \n",
       "1      1           7.0        0.3458   \n",
       "2      2           7.0        0.3102   \n",
       "3      3           8.0        0.5949   \n",
       "4      4          12.0        0.3489   \n",
       "5      5           7.0        0.7417   \n",
       "6      6           3.0        0.3175   \n",
       "7      7           6.0        0.4179   \n",
       "8      8           6.0        0.5565   \n",
       "9      9          18.0        0.4230   \n",
       "\n",
       "                                            각 토픽의 비중  \n",
       "0  [(2, 0.2929493), (3, 0.19900009), (6, 0.150440...  \n",
       "1  [(3, 0.100675434), (6, 0.1955454), (7, 0.34582...  \n",
       "2  [(2, 0.23782076), (3, 0.13286923), (6, 0.30618...  \n",
       "3  [(3, 0.061086643), (6, 0.1791152), (8, 0.59487...  \n",
       "4  [(4, 0.21355851), (6, 0.33849853), (12, 0.3489...  \n",
       "5  [(7, 0.7417126), (12, 0.058767177), (18, 0.160...  \n",
       "6  [(3, 0.31747434), (6, 0.1430436), (11, 0.08233...  \n",
       "7  [(3, 0.18095222), (6, 0.41792268), (7, 0.19717...  \n",
       "8  [(3, 0.26006368), (6, 0.55646414), (18, 0.1583...  \n",
       "9  [(2, 0.036640637), (6, 0.18301511), (8, 0.0590...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_table = make_topic_table_per_doc(ldamodel, corpus, tokenized_doc)\n",
    "topic_table = topic_table.reset_index()\n",
    "topic_table.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\n",
    "topic_table[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-gpu",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
