{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Competetion\n",
    "# Bag of Words meets Bag of Popcorn\n",
    "## Tutorial 3 - K-means 군집화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1f893224d88>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec.load('model/02.300features_40minwords_10text')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 숫자로 단어를 표현\n",
    "# Word2Vec 모델은 어휘의 각 단어에 대한 feature 벡터로 구성되며 \n",
    "# 'syn0'이라는 넘파이 배열로 저장된다.\n",
    "# syn0의 행 수는 모델 어휘의 단어 수\n",
    "# 컬럼 수는 2 부에서 설정 한 피처 벡터의 크기\n",
    "type(model.wv.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11986, 300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syn0의 행 수는 모델 어휘의 단어 수\n",
    "# 열 수는 2부에서 설정한 특징 벡터의 크기\n",
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개별 단어 벡터 접근\n",
    "model.wv['flower'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09226657, -0.0415086 , -0.10348549, -0.0360752 , -0.03338521,\n",
       "        0.03581364, -0.06411014,  0.0577444 , -0.09917627,  0.02938064],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['flower'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means (K평균)클러스터링으로 데이터 묶기\n",
    "\n",
    "* [K-평균 알고리즘 - 위키백과, 우리 모두의 백과사전](https://ko.wikipedia.org/wiki/K-%ED%8F%89%EA%B7%A0_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)\n",
    "\n",
    "- 클러스터링은 비지도 학습 기법\n",
    "- 클러스터링은 유사성 등 개념에 기초해 몇몇 그룹으로 분류하는 기법\n",
    "- 클러스터링의 목적은 샘플(실수로 구성된 n차원의 벡터)을 내부적으로는 비슷하지만 외부적으로 공통 분모가 없는 여러 그룹으로 묶는 것\n",
    "- 특정 차원의 범위가 다른 차원과 차이가 크면 클러스터링 하기 전에 스케일을 조정해야 한다.\n",
    "\n",
    "    1. 최초 센트로이드(centroid)(중심점)로 k개의 벡터를 무작위로 선정한다.\n",
    "    2. 각 샘플을 그 위치에서 가장 가까운 센트로이드에 할당한다.\n",
    "    3. 센트로이드의 위치를 재계산한다.\n",
    "    4. 센트로이드가 더 이상 움직이지 않을 때까지 2와 3을 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  805.195728302002 seconds.\n"
     ]
    }
   ],
   "source": [
    "# 단어 벡터에서 k-means를 실행하고 일부 클러스터를 찍어본다.\n",
    "start = time.time() # 시작시간\n",
    "\n",
    "# 클러스터의 크기 \"k\"를 어휘 크기의 1/5 이나 평균 5단어로 설정한다.\n",
    "word_vectors = model.wv.syn0 # 어휘의 feature vector\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "num_clusters = int(num_clusters)\n",
    "\n",
    "# K means 를 정의하고 학습시킨다.\n",
    "kmeans_clustering = KMeans(n_clusters = num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "# 끝난시간에서 시작시간을 빼서 걸린 시간을 구한다.\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['nurs', 'housewif', 'housekeep', 'iri', 'handyman', 'gracious']\n",
      "\n",
      "Cluster 1\n",
      "['garish', 'anachronist', 'gaudi']\n",
      "\n",
      "Cluster 2\n",
      "['bomb', 'engin', 'helicopt', 'airplan', 'tank', 'rocket', 'aircraft', 'laser', 'missil', 'submarin', 'airlin', 'stealth', 'chopper', 'saucer', 'deton', 'torpedo', 'launcher', 'crossbow', 'malfunct', 'cockpit']\n",
      "\n",
      "Cluster 3\n",
      "['press', 'offici', 'defect', 'sponsor', 'oversea', 'pr', 'committe', 'congress', 'chairman', 'urgent', 'affili', 'survey']\n",
      "\n",
      "Cluster 4\n",
      "['hitchcock', 'alfr', 'kurosawa', 'polanski', 'chabrol', 'visconti', 'vertigo']\n",
      "\n",
      "Cluster 5\n",
      "['expert', 'intimid', 'conspir', 'unscrupul', 'emin', 'opportunist', 'ir', 'embezzl', 'unsavori', 'crafti', 'embitt', 'upright', 'incur', 'accumul']\n",
      "\n",
      "Cluster 6\n",
      "['frank', 'scott', 'oliv', 'ian', 'brando', 'matthau', 'randolph', 'beatti', 'reliabl', 'mcdowel', 'everett', 'alec', 'scroog', 'dustin', 'rupert', 'colman', 'voight', 'meredith', 'guin', 'coleman', 'burgess', 'oat', 'fishburn', 'holliday', 'corki', 'bogard', 'schildkraut']\n",
      "\n",
      "Cluster 7\n",
      "['childish', 'juvenil', 'infantil']\n",
      "\n",
      "Cluster 8\n",
      "['peac', 'freedom']\n",
      "\n",
      "Cluster 9\n",
      "['antiqu', 'epidem', 'aborigin', 'frenchman', 'milit', 'eskimo']\n"
     ]
    }
   ],
   "source": [
    "# 각 어휘 단어를 클러스터 번호에 매핑되게 word/Index 사전을 만든다.\n",
    "idx = list(idx)\n",
    "names = model.wv.index2word\n",
    "word_centroid_map = {names[i]: idx[i] for i in range(len(names))}\n",
    "#     word_centroid_map = dict(zip( model.wv.index2word, idx ))\n",
    "\n",
    "# 첫번째 클러스터의 처음 10개를 출력\n",
    "for cluster in range(0,10):\n",
    "    # 클러스터 번호를 출력\n",
    "    print(\"\\nCluster {}\".format(cluster))\n",
    "    \n",
    "    # 클러스터번호와 클러스터에 있는 단어를 찍는다.\n",
    "    words = []\n",
    "    for i in range(0, len(list(word_centroid_map.values()))):\n",
    "        if(list(word_centroid_map.values())[i] == cluster):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/labeledTrainData.tsv', \n",
    "                    header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv('data/testData.tsv', \n",
    "                   header=0, delimiter=\"\\t\", quoting=3)\n",
    "# unlabeled_train = pd.read_csv( 'data/unlabeledTrainData.tsv', header=0,  delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "# 학습 리뷰를 정제한다.\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append(\n",
    "        KaggleWord2VecUtility.review_to_wordlist(review, remove_stopwords=True ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 리뷰를 정제한다.\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(\n",
    "        KaggleWord2VecUtility.review_to_wordlist(review, remove_stopwords=True ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bags of centroids 생성\n",
    "# 속도를 위해 centroid 학습 세트 bag을 미리 할당\n",
    "train_centroids = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "train_centroids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroid 는 두 클러스터의 중심점을 정의 한 다음 중심점의 거리를 측정한 것\n",
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    \n",
    "    # 클러스터의 수는 word / centroid map에서 가장 높은 클러스트 인덱스와 같다.\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    \n",
    "    # 속도를 위해 bag of centroids vector를 미리 할당한다.\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    \n",
    "    # 루프를 돌며 단어가 word_centroid_map에 있다면\n",
    "    # 해당되는 클러스터의 수를 하나씩 증가시켜 준다.\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    \n",
    "    # bag of centroids를 반환한다.\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n",
      "Wall time: 37.6 s\n"
     ]
    }
   ],
   "source": [
    "# 학습 리뷰를 bags of centroids 로 변환\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, word_centroid_map)\n",
    "    counter += 1\n",
    "\n",
    "# 테스트 리뷰도 같은 방법으로 반복\n",
    "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, word_centroid_map)\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "# 랜덤포레스트를 사용하여 학습시키고 예측\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# train 데이터의 레이블을 통해 학습시키고 예측\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "%time forest = forest.fit(train_centroids, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 9s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "%time score = np.mean(cross_val_score(\\\n",
    "    forest, train_centroids, train['sentiment'], cv=10,\\\n",
    "    scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = forest.predict(test_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912419008"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 csv로 저장\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(\"data/03.submit_BagOfCentroids_{0:.5f}.csv\".format(score), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle competition score: 0.83632**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2)\n",
    "fig.set_size_inches(12,5)\n",
    "sns.countplot(train['sentiment'], ax=axes[0])\n",
    "sns.countplot(output['sentiment'], ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    12708\n",
       "1    12292\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sentiment = output['sentiment'].value_counts()\n",
    "print(output_sentiment[0] - output_sentiment[1])\n",
    "output_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
